{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c4ffd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install beautifulsoup4\n",
    "!pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda7ee25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "import subprocess\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21efc96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSessions(year, month):\n",
    "    # Status messsages which month will be currently processed\n",
    "    print(\"Getting sessions of the month \" + str(month) + \" from year \" + str(year) + \" ...\")\n",
    "    \n",
    "    url_string = 'https://parlamentsinfo.giessen.de/si0040.php?__cjahr=' + str(year) + '&__cmonat=' + str(month) + '&__canz=1&__cselect=0'\n",
    "    f = urllib.request.urlopen(url_string)\n",
    "    \n",
    "    html = f.read().decode('utf-8')\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    currentWeekDay = None\n",
    "    \n",
    "    # Saves all session elements\n",
    "    list_of_session_elements = []\n",
    "    \n",
    "    archive_link = None\n",
    "    \n",
    "    # Tries to archive the url to the internet archive and save the link\n",
    "    try:\n",
    "        archive_link = archive_url(url_string)\n",
    "    except BaseException:\n",
    "        print(\"Could not get an archive link for: \" + url_string)\n",
    "    \n",
    "    for tr_element in soup.find_all('tr'):\n",
    "        session_information = {}\n",
    "        \n",
    "        tr_element = str(tr_element)\n",
    "        \n",
    "        soup2 = BeautifulSoup(tr_element, 'html.parser')\n",
    "        span_element = soup2.find('span',{\"class\": \"weekday\"})\n",
    "        \n",
    "        if (span_element != None):\n",
    "            currentWeekDay = span_element.getText()\n",
    "            \n",
    "        date_string = currentWeekDay + \".\" + str(month) + \".\" + str(year)\n",
    "        session_information[\"session_date\"] = date_string\n",
    "            \n",
    "        soup3 = BeautifulSoup(tr_element, 'html.parser')\n",
    "        div_element = soup3.find('div', {\"class\": \"smc-el-h\"})\n",
    "        \n",
    "        soup4 = BeautifulSoup(tr_element, 'html.parser')\n",
    "        li_element = soup4.find('li', {\"class\": \"list-inline-item\"})\n",
    "        \n",
    "        if (li_element != None):\n",
    "            times = re.findall('[0-9][0-9]:[0-9][0-9]', li_element.getText())\n",
    "\n",
    "            if (len(times) == 1):\n",
    "                session_information[\"session_starttime\"] = times[0]\n",
    "            elif (len(times) == 2):\n",
    "                session_information[\"session_starttime\"] = times[0]\n",
    "                session_information[\"session_endtime\"] = times[1]\n",
    "    \n",
    "        if (div_element != None):\n",
    "            div_element_text = str(div_element)\n",
    "            soup5 = BeautifulSoup(div_element_text, 'html.parser')\n",
    "            link_element = soup5.find('a', {\"class\": \"smce-a-u smc-link-normal smc_doc smc_datatype_si\"})\n",
    "            \n",
    "            name_string = div_element.getText()\n",
    "            session_information[\"session_name\"] = name_string\n",
    "            \n",
    "            now = datetime.now()\n",
    "            reference_access_date = now.strftime(\"%d.%m.%Y\") \n",
    "            \n",
    "            session_information[\"reference\"] = url_string\n",
    "            session_information[\"reference_access_date\"] = reference_access_date\n",
    "            \n",
    "            if (archive_link != None):\n",
    "                session_information[\"reference_archive_link\"] = archive_link\n",
    "\n",
    "            if (link_element != None):\n",
    "                session_information[\"link\"] = \"https://parlamentsinfo.giessen.de/\" + link_element['href'] \n",
    "                \n",
    "            list_of_session_elements.append(session_information)\n",
    "                \n",
    "    # Return a list with all sessions of the month\n",
    "    return list_of_session_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6882fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_session_details(url_to_session):\n",
    "    f = urllib.request.urlopen(url_to_session)\n",
    "    \n",
    "    html = f.read().decode('utf-8')\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    session_id = soup.find('div', {\"class\": \"smc-dg-td-2 smc-table-cell siname\"}).getText()\n",
    "    session_name = soup.find('div', {\"class\": \"smc-dg-td-2 smc-table-cell sigrname\"}).getText()\n",
    "    session_date = soup.find('div', {\"class\": \"smc-dg-td-2 smc-table-cell sidat\"}).getText()\n",
    "    session_start_end = soup.find('div', {\"class\": \"smc-dg-td-2 smc-table-cell yytime\"}).getText()\n",
    "            \n",
    "    now = datetime.now()\n",
    "    reference_date = now.strftime(\"%d.%m.%Y\") \n",
    "    \n",
    "    session_information = {\"session_id\": session_id,\n",
    "                           \"session_name\": session_name,\n",
    "                           \"session_date\": session_date,\n",
    "                           \"reference\": url_to_session,\n",
    "                           \"reference_access_date\": reference_date\n",
    "                          }\n",
    "    \n",
    "    # Tries to archive the url to the internet archive and save the link\n",
    "    try:\n",
    "        archive_link = archive_url(url_to_session)\n",
    "        session_information[\"reference_archive_link\"] = archive_link\n",
    "    except BaseException:\n",
    "        print(\"Could not get an archive link for: \" + url_to_session)\n",
    "\n",
    "    times = re.findall('[0-9][0-9]:[0-9][0-9]', session_start_end)\n",
    "    \n",
    "    if (len(times) == 1):\n",
    "        session_information[\"session_starttime\"] = times[0]\n",
    "    elif (len(times) == 2):\n",
    "        session_information[\"session_starttime\"] = times[0]\n",
    "        session_information[\"session_endtime\"] = times[1]\n",
    "    \n",
    "    agenda = []\n",
    "    \n",
    "    for tr_element in soup.find_all('tr', {\"class\": \"smc-t-r-l\"}):\n",
    "        tr_element = str(tr_element)\n",
    "        \n",
    "        # Saves the agenda item with a proposal\n",
    "        agenda_item = {}\n",
    "        \n",
    "        soup2 = BeautifulSoup(tr_element, 'html.parser')\n",
    "        span_element = soup2.find('span',{\"class\": \"badge\"})\n",
    "        \n",
    "        if (span_element == None):\n",
    "            continue\n",
    "        \n",
    "        span_element_text = span_element.getText()\n",
    "        \n",
    "        result_order = re.findall('[0-9]+', span_element_text)\n",
    "        \n",
    "        if (len(result_order) > 0):\n",
    "            agenda_item[\"order\"] = int(result_order[0])\n",
    "        \n",
    "       \n",
    "        \n",
    "        if (re.findall('(Ö|N){1}', span_element_text))[0] == \"Ö\":\n",
    "            agenda_item[\"public_status\"] = True\n",
    "        elif (re.findall('(Ö|N){1}', span_element_text))[0] == \"N\":\n",
    "            agenda_item[\"public_status\"] = False\n",
    "        \n",
    "            \n",
    "        soup3 = BeautifulSoup(tr_element, 'html.parser')\n",
    "        link_to_proposal = soup3.find('a',{\"class\": \"smce-a-u smc-link-procedure smc_doc smc_field_voname smcnowrap smc_datatype_vo\"})\n",
    "        \n",
    "        if (link_to_proposal == None):\n",
    "            continue\n",
    "            \n",
    "        url_to_proposal = \"https://parlamentsinfo.giessen.de/\" + link_to_proposal['href']\n",
    "        agenda_item[\"url_to_proposal\"] = url_to_proposal\n",
    "            \n",
    "        agenda.append(agenda_item)\n",
    "        \n",
    "    \n",
    "    session_information[\"agenda\"] = agenda    \n",
    "        \n",
    "            \n",
    "    return session_information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b77504",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Funktion reads the propsal information from the webpage of the parlament information system session.net\n",
    "and saves them into an JSON object and returns it.\n",
    "'''\n",
    "def get_proposal_details(url_to_proposal):\n",
    "    print(\"Fetching information of \" + url_to_proposal + \" ...\")\n",
    "    \n",
    "    # Saves the details of the proposal\n",
    "    proposal_information = {}\n",
    "    \n",
    "    f = urllib.request.urlopen(url_to_proposal)\n",
    "    \n",
    "    html = f.read().decode('utf-8')\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    proposal_subject = soup.find('div', {\"class\": \"smc-dg-td-2 smc-table-cell vobetr\"})\n",
    "    proposal_number = soup.find('div', {\"class\": \"smc-dg-td-2 smc-table-cell voname\"})\n",
    "    proposal_filenumber = soup.find('div', {\"class\": \"smc-dg-td-2 smc-table-cell voakz\"})\n",
    "    \n",
    "    # Catches the parlament information link\n",
    "    proposal_information[\"session_net_link\"] = url_to_proposal\n",
    "    \n",
    "    # Process the subject, proposal_type and the proposal date\n",
    "    if proposal_subject != None:\n",
    "        proposal_subject = str(proposal_subject.getText())\n",
    "        proposal_subject_array = proposal_subject.split(\"-\")\n",
    "        \n",
    "        # Checks that all pieces are existing\n",
    "        if (len(proposal_subject_array) >= 3):\n",
    "            \n",
    "            # Catches the proposal subject\n",
    "            subject = proposal_subject_array[0:-2]\n",
    "            proposal_information[\"proposal_subject\"] = (\"\".join(subject)).strip()\n",
    "            \n",
    "            # Catches the proposal type\n",
    "            authors = proposal_subject_array[-2]\n",
    "            proposal_type = re.findall(\"(Antrag|Anfrage)\", authors)\n",
    "            if (len(proposal_type) > 0):\n",
    "                if proposal_type[0] == \"Antrag\":\n",
    "                    proposal_information[\"proposal_type\"] = \"Antrag\"\n",
    "                elif proposal_type[0] == \"Anfrage\":\n",
    "                    proposal_information[\"proposal_type\"] = \"Anfrage\"\n",
    "                    \n",
    "            \n",
    "            # Catches the proposal date\n",
    "            authors = proposal_subject_array[-2]\n",
    "            proposal_date = re.findall(\"[0-9][0-9]\\.[0-9][0-9]\\.[0-9][0-9][0-9][0-9]\", authors)\n",
    "            if (len(proposal_date) > 0):\n",
    "                proposal_information[\"proposal_date\"] = proposal_date[0]\n",
    "                \n",
    "     \n",
    "    # Process the propsal number\n",
    "    if proposal_number != None:\n",
    "        proposal_information[\"proposal_number\"] = proposal_number.getText()\n",
    "    \n",
    "    \n",
    "    # Process the proppsal filenumber\n",
    "    if proposal_filenumber != None:\n",
    "        proposal_information[\"proposal_filenumber\"] = proposal_filenumber.getText()\n",
    "        \n",
    "    \n",
    "    # Tries to archive the url to the internet archive and save the link\n",
    "    try:\n",
    "        archive_link = archive_url(url_to_proposal)\n",
    "        proposal_information[\"reference_archive_link\"] = archive_link\n",
    "    except BaseException:\n",
    "        print(\"Could not get an archive link for: \" + url_to_proposal)\n",
    "    \n",
    "    \n",
    "    # Calculate the date of the access of the page as a reference\n",
    "    reference_access_date = datetime.now().strftime(\"%d.%m.%Y\")\n",
    "    \n",
    "    proposal_information[\"reference_url\"] = url_to_proposal\n",
    "    proposal_information[\"reference_access_date\"] = reference_access_date\n",
    "    \n",
    "    return proposal_information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8715346c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Archives the url to the Internet Archive\n",
    "'''\n",
    "def archive_url(url):\n",
    "    command = \"wget --spider 'https://web.archive.org/save/\" + url + \"'\"\n",
    "    \n",
    "    for i in range(0,5):\n",
    "        # Increase the waiting time to avoid too many attempts in a short time\n",
    "        waiting_time = 2 * i * 15\n",
    "        time.sleep(waiting_time)\n",
    "    \n",
    "        process = subprocess.Popen(command, \n",
    "                                   stdout = subprocess.PIPE,\n",
    "                                   stderr = subprocess.PIPE,\n",
    "                                   text = True,\n",
    "                                   shell = True\n",
    "                                  )\n",
    "\n",
    "        std_out, std_err = process.communicate()\n",
    "\n",
    "        resultRegEx = re.findall(\"https:\\/\\/web.archive\\.org\\/web\\/[0-9]{14}\\/\", std_err.strip())\n",
    "\n",
    "        if (len(resultRegEx) > 1):\n",
    "            return resultRegEx[0] + url\n",
    "    \n",
    "    # Raise an exception if the webpage can not be archived\n",
    "    raise ValueError(\"No archive url was generated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c2cd07",
   "metadata": {},
   "source": [
    "## Schritt 1: Ermittelt alle Gremiensitzungen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513edcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_all_sessions = []\n",
    "\n",
    "for year in range(2000,2023):\n",
    "    for month in range(1,13):\n",
    "        # Tries to fetch all sessions from the month\n",
    "        try:\n",
    "            list_of_all_sessions = list_of_all_sessions + getSessions(year, month)\n",
    "        except BaseException:\n",
    "            print(\"Could not fetch sessions from month \" + str(month) + \" of year \" + str(year))\n",
    "        \n",
    "        # Waits a random time to prevent DDOS protection is triggered\n",
    "        waiting_time = random.randint(10,60)\n",
    "        time.sleep(waiting_time)\n",
    "\n",
    "# Saves all sessions in a JSON file\n",
    "with open('sessions_list.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(list_of_all_sessions, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de361cb",
   "metadata": {},
   "source": [
    "## Schritt 2: Ermittelt die Details zu den Gremiensitzungen und zugehörige Anträge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b373cae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opens the JSON-file with the sessions\n",
    "file_pointer = open(\"sessions_list.json\")\n",
    "\n",
    "# Reads the JSON-file with the sessions\n",
    "data = json.load(file_pointer)\n",
    "\n",
    "list_of_all_sessions = []\n",
    "\n",
    "# Iterates through the sessions\n",
    "for session in data:\n",
    "    \n",
    "    print(\"Get information from \" + session[\"session_name\"] + \" (\" + session[\"session_date\"] + \") ...\")\n",
    "    \n",
    "    # Checks, if the session has a link\n",
    "    if 'link' in session.keys():\n",
    "        try:\n",
    "            list_of_all_sessions.append(get_session_details(session['link']))\n",
    "        except BaseException:\n",
    "            print(\"Could not get the session details of: \" + session['link'])\n",
    "          \n",
    "        # Waits a random time to prevent DDOS protection is triggered\n",
    "        waiting_time = random.randint(0, 60)\n",
    "        time.sleep(waiting_time)\n",
    "    else:\n",
    "        list_of_all_sessions.append(session)\n",
    "        \n",
    "        \n",
    "#Unique list of proposal links\n",
    "list_of_proposal_links = []\n",
    "\n",
    "\n",
    "# TODO: Fix comparisons of the links\n",
    "for session in list_of_all_sessions:\n",
    "    if 'agenda' in session.keys():\n",
    "        for agenda_item in session[\"agenda\"]:\n",
    "            if agenda_item[\"url_to_proposal\"] not in list_of_proposal_links:\n",
    "                list_of_proposal_links.append({\"url_to_proposal\":agenda_item[\"url_to_proposal\"]})\n",
    "\n",
    "\n",
    "# Saves the sessions with the details in a JSON-file\n",
    "with open('sessions_details_list.json', 'w', encoding='utf-8') as file_pointer:\n",
    "    json.dump(list_of_all_sessions, file_pointer, ensure_ascii=False, indent=4)\n",
    "    \n",
    "# Saves the links of the proposal in a JSON-file\n",
    "with open('proposal_link_list.json', 'w', encoding='utf-8') as file_pointer:\n",
    "    json.dump(list_of_proposal_links, file_pointer, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c93655",
   "metadata": {},
   "source": [
    "## Schritt 3: Ermittelt alle Details zu den einzelnen Einträgen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7373d3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opens the JSON-file with the proposals links\n",
    "file_pointer = open(\"proposal_link_list.json\")\n",
    "\n",
    "# Reads the data from the JSON-file with the proposal links\n",
    "data = json.load(file_pointer)\n",
    "\n",
    "list_of_all_proposals_with_details = []\n",
    "\n",
    "for proposal in data:\n",
    "    try:\n",
    "        proposal_with_details = get_proposal_details(proposal[\"url_to_proposal\"])\n",
    "        list_of_all_proposals_with_details.append(proposal_with_details)\n",
    "    except BaseException:\n",
    "        print(\"Error: Could not get details of the proposal: \" + proposal[\"url_to_proposal\"])\n",
    "    \n",
    "    # Waits a random time to prevent DDOS protection is triggered\n",
    "    waiting_time = random.randint(0, 30)\n",
    "    time.sleep(waiting_time)\n",
    "    \n",
    "with open('proposals_with_details.json', 'w', encoding='utf-8') as file_pointer:\n",
    "    json.dump(list_of_all_proposals_with_details, file_pointer, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3fb832",
   "metadata": {},
   "source": [
    "## Schritt 4: Erstellung einer JSON-Datei für den Antrags-Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e74f6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b503182",
   "metadata": {},
   "source": [
    "## Schritt 5: Erstellung einer JSON-Datei für den Gremiensitzungs-Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6fcb8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0b691a11",
   "metadata": {},
   "source": [
    "## Schritt 6: Import der Daten in die Wikibase-Datenbank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c19f5b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
