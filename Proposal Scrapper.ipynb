{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c4ffd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install beautifulsoup4\n",
    "!pip install lxml\n",
    "!pip install wikibaseintegrator==0.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda7ee25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "import subprocess\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from time import mktime\n",
    "\n",
    "# Imports for the WikibaseIntegrator\n",
    "from wikibaseintegrator import WikibaseIntegrator\n",
    "from wikibaseintegrator import wbi_login as wbi_login\n",
    "from wikibaseintegrator.wbi_config import config as wbi_config\n",
    "from wikibaseintegrator.datatypes import Item, String, Time, URL\n",
    "from wikibaseintegrator.wbi_enums import WikibaseDatePrecision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21efc96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSessions(year, month):\n",
    "    # Status messsages which month will be currently processed\n",
    "    print(\"Getting sessions of the month \" + str(month) + \" from year \" + str(year) + \" ...\")\n",
    "    \n",
    "    url_string = 'https://parlamentsinfo.giessen.de/si0040.php?__cjahr=' + str(year) + '&__cmonat=' + str(month) + '&__canz=1&__cselect=0'\n",
    "    f = urllib.request.urlopen(url_string)\n",
    "    \n",
    "    html = f.read().decode('utf-8')\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    currentWeekDay = None\n",
    "    \n",
    "    # Saves all session elements\n",
    "    list_of_session_elements = []\n",
    "    \n",
    "    archive_link = None\n",
    "    \n",
    "    # Tries to archive the url to the internet archive and save the link\n",
    "    try:\n",
    "        archive_link = archive_url(url_string)\n",
    "    except BaseException:\n",
    "        print(\"Could not get an archive link for: \" + url_string)\n",
    "    \n",
    "    for tr_element in soup.find_all('tr'):\n",
    "        session_information = {}\n",
    "        \n",
    "        tr_element = str(tr_element)\n",
    "        \n",
    "        soup2 = BeautifulSoup(tr_element, 'html.parser')\n",
    "        span_element = soup2.find('span',{\"class\": \"weekday\"})\n",
    "        \n",
    "        if (span_element != None):\n",
    "            currentWeekDay = span_element.getText()\n",
    "            \n",
    "        date_string = currentWeekDay + \".\" + str(month) + \".\" + str(year)\n",
    "        session_information[\"session_date\"] = date_string\n",
    "            \n",
    "        soup3 = BeautifulSoup(tr_element, 'html.parser')\n",
    "        div_element = soup3.find('div', {\"class\": \"smc-el-h\"})\n",
    "        \n",
    "        soup4 = BeautifulSoup(tr_element, 'html.parser')\n",
    "        li_element = soup4.find('li', {\"class\": \"list-inline-item\"})\n",
    "        \n",
    "        if (li_element != None):\n",
    "            times = re.findall('[0-9][0-9]:[0-9][0-9]', li_element.getText())\n",
    "\n",
    "            if (len(times) == 1):\n",
    "                session_information[\"session_starttime\"] = times[0]\n",
    "            elif (len(times) == 2):\n",
    "                session_information[\"session_starttime\"] = times[0]\n",
    "                session_information[\"session_endtime\"] = times[1]\n",
    "    \n",
    "        if (div_element != None):\n",
    "            div_element_text = str(div_element)\n",
    "            soup5 = BeautifulSoup(div_element_text, 'html.parser')\n",
    "            link_element = soup5.find('a', {\"class\": \"smce-a-u smc-link-normal smc_doc smc_datatype_si\"})\n",
    "            \n",
    "            name_string = div_element.getText()\n",
    "            session_information[\"session_name\"] = name_string\n",
    "            \n",
    "            now = datetime.now()\n",
    "            reference_access_date = now.strftime(\"%d.%m.%Y\") \n",
    "            \n",
    "            session_information[\"reference\"] = url_string\n",
    "            session_information[\"reference_access_date\"] = reference_access_date\n",
    "            \n",
    "            if (archive_link != None):\n",
    "                session_information[\"reference_archive_link\"] = archive_link\n",
    "\n",
    "            if (link_element != None):\n",
    "                session_information[\"link\"] = \"https://parlamentsinfo.giessen.de/\" + link_element['href'] \n",
    "                \n",
    "            list_of_session_elements.append(session_information)\n",
    "                \n",
    "    # Return a list with all sessions of the month\n",
    "    return list_of_session_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6882fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_session_details(url_to_session):\n",
    "    f = urllib.request.urlopen(url_to_session)\n",
    "    \n",
    "    html = f.read().decode('utf-8')\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    session_id = soup.find('div', {\"class\": \"smc-dg-td-2 smc-table-cell siname\"}).getText()\n",
    "    session_name = soup.find('div', {\"class\": \"smc-dg-td-2 smc-table-cell sigrname\"}).getText()\n",
    "    session_date = soup.find('div', {\"class\": \"smc-dg-td-2 smc-table-cell sidat\"}).getText()\n",
    "    session_start_end = soup.find('div', {\"class\": \"smc-dg-td-2 smc-table-cell yytime\"}).getText()\n",
    "            \n",
    "    now = datetime.now()\n",
    "    reference_date = now.strftime(\"%d.%m.%Y\") \n",
    "    \n",
    "    session_information = {\"session_id\": session_id,\n",
    "                           \"session_name\": session_name,\n",
    "                           \"session_date\": session_date,\n",
    "                           \"reference\": url_to_session,\n",
    "                           \"reference_access_date\": reference_date\n",
    "                          }\n",
    "    \n",
    "    # Tries to archive the url to the internet archive and save the link\n",
    "    try:\n",
    "        archive_link = archive_url(url_to_session)\n",
    "        session_information[\"reference_archive_link\"] = archive_link\n",
    "    except BaseException:\n",
    "        print(\"Could not get an archive link for: \" + url_to_session)\n",
    "\n",
    "    times = re.findall('[0-9][0-9]:[0-9][0-9]', session_start_end)\n",
    "    \n",
    "    if (len(times) == 1):\n",
    "        session_information[\"session_starttime\"] = times[0]\n",
    "    elif (len(times) == 2):\n",
    "        session_information[\"session_starttime\"] = times[0]\n",
    "        session_information[\"session_endtime\"] = times[1]\n",
    "    \n",
    "    agenda = []\n",
    "    \n",
    "    for tr_element in soup.find_all('tr', {\"class\": \"smc-t-r-l\"}):\n",
    "        tr_element = str(tr_element)\n",
    "        \n",
    "        # Saves the agenda item with a proposal\n",
    "        agenda_item = {}\n",
    "        \n",
    "        soup2 = BeautifulSoup(tr_element, 'html.parser')\n",
    "        span_element = soup2.find('span',{\"class\": \"badge\"})\n",
    "        \n",
    "        if (span_element == None):\n",
    "            continue\n",
    "        \n",
    "        span_element_text = span_element.getText()\n",
    "        \n",
    "        result_order = re.findall('[0-9]+', span_element_text)\n",
    "        \n",
    "        if (len(result_order) > 0):\n",
    "            agenda_item[\"order\"] = int(result_order[0])\n",
    "        \n",
    "       \n",
    "        \n",
    "        if (re.findall('(Ö|N){1}', span_element_text))[0] == \"Ö\":\n",
    "            agenda_item[\"public_status\"] = True\n",
    "        elif (re.findall('(Ö|N){1}', span_element_text))[0] == \"N\":\n",
    "            agenda_item[\"public_status\"] = False\n",
    "        \n",
    "            \n",
    "        soup3 = BeautifulSoup(tr_element, 'html.parser')\n",
    "        link_to_proposal = soup3.find('a',{\"class\": \"smce-a-u smc-link-procedure smc_doc smc_field_voname smcnowrap smc_datatype_vo\"})\n",
    "        \n",
    "        if (link_to_proposal == None):\n",
    "            continue\n",
    "            \n",
    "        url_to_proposal = \"https://parlamentsinfo.giessen.de/\" + link_to_proposal['href']\n",
    "        agenda_item[\"url_to_proposal\"] = url_to_proposal\n",
    "            \n",
    "        agenda.append(agenda_item)\n",
    "        \n",
    "    \n",
    "    session_information[\"agenda\"] = agenda    \n",
    "        \n",
    "            \n",
    "    return session_information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b77504",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Funktion reads the propsal information from the webpage of the parlament information system session.net\n",
    "and saves them into an JSON object and returns it.\n",
    "'''\n",
    "def get_proposal_details(url_to_proposal):\n",
    "    print(\"Fetching information of \" + url_to_proposal + \" ...\")\n",
    "    \n",
    "    # Saves the details of the proposal\n",
    "    proposal_information = {}\n",
    "    \n",
    "    f = urllib.request.urlopen(url_to_proposal)\n",
    "    \n",
    "    html = f.read().decode('utf-8')\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    proposal_subject = soup.find('div', {\"class\": \"smc-dg-td-2 smc-table-cell vobetr\"})\n",
    "    proposal_number = soup.find('div', {\"class\": \"smc-dg-td-2 smc-table-cell voname\"})\n",
    "    proposal_filenumber = soup.find('div', {\"class\": \"smc-dg-td-2 smc-table-cell voakz\"})\n",
    "    \n",
    "    # Catches the parlament information link\n",
    "    proposal_information[\"session_net_link\"] = url_to_proposal\n",
    "    \n",
    "    # Process the subject, proposal_type and the proposal date\n",
    "    if proposal_subject != None:\n",
    "        proposal_subject = str(proposal_subject.getText())\n",
    "        proposal_subject_array = proposal_subject.split(\"-\")\n",
    "        \n",
    "        # Checks that all pieces are existing\n",
    "        if (len(proposal_subject_array) >= 3):\n",
    "            \n",
    "            # Catches the proposal subject\n",
    "            subject = proposal_subject_array[0:-2]\n",
    "            proposal_information[\"proposal_subject\"] = (\"\".join(subject)).strip()\n",
    "            \n",
    "            # Catches the proposal type\n",
    "            authors = proposal_subject_array[-2]\n",
    "            proposal_type = re.findall(\"(Antrag|Anfrage)\", authors)\n",
    "            if (len(proposal_type) > 0):\n",
    "                if proposal_type[0] == \"Antrag\":\n",
    "                    proposal_information[\"proposal_type\"] = \"Antrag\"\n",
    "                elif proposal_type[0] == \"Anfrage\":\n",
    "                    proposal_information[\"proposal_type\"] = \"Anfrage\"\n",
    "                    \n",
    "            \n",
    "            # Catches the proposal date\n",
    "            authors = proposal_subject_array[-2]\n",
    "            proposal_date = re.findall(\"[0-9][0-9]\\.[0-9][0-9]\\.[0-9][0-9][0-9][0-9]\", authors)\n",
    "            if (len(proposal_date) > 0):\n",
    "                proposal_information[\"proposal_date\"] = proposal_date[0]\n",
    "                \n",
    "     \n",
    "    # Process the propsal number\n",
    "    if proposal_number != None:\n",
    "        proposal_information[\"proposal_number\"] = proposal_number.getText()\n",
    "    \n",
    "    \n",
    "    # Process the proppsal filenumber\n",
    "    if proposal_filenumber != None:\n",
    "        proposal_information[\"proposal_filenumber\"] = proposal_filenumber.getText()\n",
    "        \n",
    "    \n",
    "    # Tries to archive the url to the internet archive and save the link\n",
    "    try:\n",
    "        archive_link = archive_url(url_to_proposal)\n",
    "        proposal_information[\"reference_archive_link\"] = archive_link\n",
    "    except BaseException:\n",
    "        print(\"Could not get an archive link for: \" + url_to_proposal)\n",
    "    \n",
    "    \n",
    "    # Calculate the date of the access of the page as a reference\n",
    "    reference_access_date = datetime.now().strftime(\"%d.%m.%Y\")\n",
    "    \n",
    "    proposal_information[\"reference_url\"] = url_to_proposal\n",
    "    proposal_information[\"reference_access_date\"] = reference_access_date\n",
    "    \n",
    "    return proposal_information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8715346c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Archives the url to the Internet Archive\n",
    "'''\n",
    "def archive_url(url):\n",
    "    command = \"wget --spider 'https://web.archive.org/save/\" + url + \"'\"\n",
    "    \n",
    "    for i in range(0,5):\n",
    "        # Increase the waiting time to avoid too many attempts in a short time\n",
    "        waiting_time = 2 * i * 15\n",
    "        time.sleep(waiting_time)\n",
    "    \n",
    "        process = subprocess.Popen(command, \n",
    "                                   stdout = subprocess.PIPE,\n",
    "                                   stderr = subprocess.PIPE,\n",
    "                                   text = True,\n",
    "                                   shell = True\n",
    "                                  )\n",
    "\n",
    "        std_out, std_err = process.communicate()\n",
    "\n",
    "        resultRegEx = re.findall(\"https:\\/\\/web.archive\\.org\\/web\\/[0-9]{14}\\/\", std_err.strip())\n",
    "\n",
    "        if (len(resultRegEx) > 1):\n",
    "            return resultRegEx[0] + url\n",
    "    \n",
    "    # Raise an exception if the webpage can not be archived\n",
    "    raise ValueError(\"No archive url was generated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7702e962",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function gets the link to the calendar page which links to the session\n",
    "'''\n",
    "def get_reference_link_by_session_link(session_link):\n",
    "    # Opens the JSON-file with the sessions\n",
    "    file_pointer = open(\"sessions_list.json\")\n",
    "\n",
    "    # Reads the JSON-file with the sessions\n",
    "    data = json.load(file_pointer)\n",
    "    \n",
    "    for session in data:\n",
    "        if session[\"link\"] == session_link:\n",
    "            return session[\"reference\"]\n",
    "    \n",
    "    # Raises an exception if no reference link could be found\n",
    "    raise NoValueError(\"Could not find a linking reference for: \" + session_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e585d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function geth the link to a session page which links to the proposal\n",
    "'''\n",
    "def get_reference_link_by_proposal_link(propopsal_link):\n",
    "    # Opens the JSON-file with the sessions\n",
    "    file_pointer = open(\"sessions_details_list.json\")\n",
    "    \n",
    "    # Reads the JSON-file with the sessions\n",
    "    data = json.load(file_pointer)\n",
    "    \n",
    "    for session in data:\n",
    "        # Checks if the session contains an agenda\n",
    "        if 'agenda' in session.keys():\n",
    "            \n",
    "            # Iterates through the agenda items\n",
    "            for agenda_item in session[\"agenda\"]:\n",
    "                \n",
    "                # Returns the session link with further information as a reference link for the proposal\n",
    "                if agenda_item[\"url_to_proposal\"] == propopsal_link:\n",
    "                    return session[\"reference\"], session[\"reference_archive_link\"], session[\"reference_access_date\"]\n",
    "                \n",
    "    # Raises an exception if no reference link could be found\n",
    "    raise NoValueError(\"Could not find a linking reference for: \" + propopsal_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd7d86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function returns the login credentials from the config file\n",
    "'''\n",
    "def get_login_credentials():\n",
    "    file_pointer = open(\"config.json\")\n",
    "    \n",
    "    # Reads the data from the config file\n",
    "    data = json.load(file_pointer)\n",
    "    \n",
    "    return data[\"login_name\"], data[\"login_password\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02174bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function return the server api address credentials from the config file \n",
    "'''\n",
    "def get_server_address_config():\n",
    "    file_pointer = open(\"config.json\")\n",
    "    \n",
    "    # Reads the data from the config file\n",
    "    data = json.load(file_pointer)\n",
    "    \n",
    "    return data[\"mediawiki_api\"], data[\"sparql_endpoint_url\"], data[\"wikibase_url\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416ab983",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function get the reference\n",
    "'''\n",
    "def get_reference_parlamentsinfosystem(reference_url, archive_url, retrieved_at):\n",
    "    reference = []\n",
    "    \n",
    "    # Adds the source of the reference\n",
    "    source = String(value=\"Parlamentsinformationssystem der Stadt Gießen\", prop_nr=\"P25\")\n",
    "    reference.append(source)                               \n",
    "                \n",
    "    # Adds the url of the reference\n",
    "    reference_url = String(value=reference_url, prop_nr=\"P12\")\n",
    "    reference.append(reference_url)\n",
    "                \n",
    "    # Adds the archive url of the reference\n",
    "    archive_url = String(value=archive_url, prop_nr=\"P14\")\n",
    "    reference.append(archive_url)\n",
    "                \n",
    "    # Adds the retrieved date of the reference\n",
    "    retrieved_date = datetime.fromtimestamp(mktime(time.strptime(retrieved_at, \"%d.%m.%Y\")))\n",
    "    retrieved_date = retrieved_date.strftime(\"+%Y-%m-%dT%H:%M:%SZ\")\n",
    "    retrieved_at = Time(time=retrieved_date, prop_nr=\"P16\", precision=WikibaseDatePrecision.DAY, references=references)\n",
    "    reference.append(retrieved_at)\n",
    "                \n",
    "    return reference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c2cd07",
   "metadata": {},
   "source": [
    "## Schritt 1: Ermittelt alle Gremiensitzungen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513edcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_all_sessions = []\n",
    "\n",
    "for year in range(2000,2023):\n",
    "    for month in range(1,13):\n",
    "        # Tries to fetch all sessions from the month\n",
    "        try:\n",
    "            list_of_all_sessions = list_of_all_sessions + getSessions(year, month)\n",
    "        except BaseException:\n",
    "            print(\"Could not fetch sessions from month \" + str(month) + \" of year \" + str(year))\n",
    "        \n",
    "        # Waits a random time to prevent DDOS protection is triggered\n",
    "        waiting_time = random.randint(10,60)\n",
    "        time.sleep(waiting_time)\n",
    "\n",
    "# Saves all sessions in a JSON file\n",
    "with open('sessions_list.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(list_of_all_sessions, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de361cb",
   "metadata": {},
   "source": [
    "## Schritt 2: Ermittelt die Details zu den Gremiensitzungen und zugehörige Anträge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b373cae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opens the JSON-file with the sessions\n",
    "file_pointer = open(\"sessions_list.json\")\n",
    "\n",
    "# Reads the JSON-file with the sessions\n",
    "data = json.load(file_pointer)\n",
    "\n",
    "list_of_all_sessions = []\n",
    "\n",
    "# Iterates through the sessions\n",
    "for session in data:\n",
    "    \n",
    "    print(\"Get information from \" + session[\"session_name\"] + \" (\" + session[\"session_date\"] + \") ...\")\n",
    "    \n",
    "    # Checks, if the session has a link\n",
    "    if 'link' in session.keys():\n",
    "        try:\n",
    "            list_of_all_sessions.append(get_session_details(session['link']))\n",
    "        except BaseException:\n",
    "            print(\"Could not get the session details of: \" + session['link'])\n",
    "          \n",
    "        # Waits a random time to prevent DDOS protection is triggered\n",
    "        waiting_time = random.randint(0, 60)\n",
    "        time.sleep(waiting_time)\n",
    "    else:\n",
    "        list_of_all_sessions.append(session)\n",
    "        \n",
    "        \n",
    "#Unique list of proposal links\n",
    "list_of_proposal_links = []\n",
    "\n",
    "\n",
    "# TODO: Fix comparisons of the links\n",
    "for session in list_of_all_sessions:\n",
    "    if 'agenda' in session.keys():\n",
    "        for agenda_item in session[\"agenda\"]:\n",
    "            if agenda_item[\"url_to_proposal\"] not in list_of_proposal_links:\n",
    "                list_of_proposal_links.append({\"url_to_proposal\":agenda_item[\"url_to_proposal\"]})\n",
    "\n",
    "\n",
    "# Saves the sessions with the details in a JSON-file\n",
    "with open('sessions_details_list.json', 'w', encoding='utf-8') as file_pointer:\n",
    "    json.dump(list_of_all_sessions, file_pointer, ensure_ascii=False, indent=4)\n",
    "    \n",
    "# Saves the links of the proposal in a JSON-file\n",
    "with open('proposal_link_list.json', 'w', encoding='utf-8') as file_pointer:\n",
    "    json.dump(list_of_proposal_links, file_pointer, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c93655",
   "metadata": {},
   "source": [
    "## Schritt 3: Ermittelt alle Details zu den einzelnen Einträgen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7373d3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opens the JSON-file with the proposals links\n",
    "file_pointer = open(\"proposal_link_list.json\")\n",
    "\n",
    "# Reads the data from the JSON-file with the proposal links\n",
    "data = json.load(file_pointer)\n",
    "\n",
    "list_of_all_proposals_with_details = []\n",
    "\n",
    "for proposal in data:\n",
    "    try:\n",
    "        proposal_with_details = get_proposal_details(proposal[\"url_to_proposal\"])\n",
    "        list_of_all_proposals_with_details.append(proposal_with_details)\n",
    "    except BaseException:\n",
    "        print(\"Error: Could not get details of the proposal: \" + proposal[\"url_to_proposal\"])\n",
    "    \n",
    "    # Waits a random time to prevent DDOS protection is triggered\n",
    "    waiting_time = random.randint(0, 30)\n",
    "    time.sleep(waiting_time)\n",
    "    \n",
    "with open('proposals_with_details.json', 'w', encoding='utf-8') as file_pointer:\n",
    "    json.dump(list_of_all_proposals_with_details, file_pointer, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3fb832",
   "metadata": {},
   "source": [
    "## Schritt 4: Erstellung einer JSON-Datei für den Antrags-Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6fcb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opens the JSON-file with the sessions\n",
    "file_pointer = open(\"proposals_with_details.json\")\n",
    "\n",
    "# Reads the JSON-file with the sessions\n",
    "data = json.load(file_pointer)\n",
    "\n",
    "\n",
    "# Saves the processed sessions for the import\n",
    "processed_proposals_for_import = []\n",
    "\n",
    "\n",
    "# Iterates through the saved proposals\n",
    "for proposal in data:\n",
    "    processed_proposal = {}\n",
    "    \n",
    "    # Generates the name of the item\n",
    "    if 'proposal_subject' in proposal.keys():\n",
    "        processed_proposal[\"name_of_item\"] = proposal[\"proposal_subject\"].strip().replace(\"\\t\",\" \")\n",
    "    else:\n",
    "        processed_proposal[\"name_of_item\"] = \"\"\n",
    "        continue\n",
    "        \n",
    "    \n",
    "    # Added the category as \"Instanz von\" property\n",
    "    processed_proposal[\"instance_of\"] = {\n",
    "                                         \"value\": \"Vorlage für städtisches Gremium\"\n",
    "                                        }\n",
    "    \n",
    "    # Adds the proposal subject as \"Betreff\" property\n",
    "    if 'proposal_subject' in proposal.keys():\n",
    "         processed_proposal[\"subject\"] = {\n",
    "                                          \"value\": proposal[\"proposal_subject\"].strip().replace(\"\\t\",\" \"),\n",
    "                                          \"reference_url\": proposal[\"reference_url\"].strip(),\n",
    "                                          \"archive_url\": proposal[\"reference_archive_link\"].strip(),\n",
    "                                          \"retrieved_at\": proposal[\"reference_access_date\"].strip() \n",
    "                                         }\n",
    "    \n",
    "    # Adds the proposal date as \"Antragsdatum\" property\n",
    "    if 'proposal_date' in proposal.keys():\n",
    "        processed_proposal[\"date\"] = {\n",
    "                                      \"value\": proposal[\"proposal_date\"].strip().replace(\"\\t\",\" \"),\n",
    "                                      \"reference_url\": proposal[\"reference_url\"].strip(),\n",
    "                                      \"archive_url\": proposal[\"reference_archive_link\"].strip(),\n",
    "                                      \"retrieved_at\": proposal[\"reference_access_date\"].strip() \n",
    "                                     }\n",
    "    \n",
    "    # Adds the proposal type as \"Typ\" property\n",
    "    if 'proposal_type' in proposal.keys():\n",
    "        processed_proposal[\"type\"] = {\n",
    "                                      \"value\": proposal[\"proposal_type\"].strip().replace(\"\\t\",\" \"),\n",
    "                                      \"reference_url\": proposal[\"reference_url\"].strip(),\n",
    "                                      \"archive_url\": proposal[\"reference_archive_link\"].strip(),\n",
    "                                      \"retrieved_at\": proposal[\"reference_access_date\"].strip() \n",
    "                                     }\n",
    "    \n",
    "    # Adds the proposal number as \"Vorlagennummer\" property\n",
    "    if 'proposal_number' in proposal.keys():\n",
    "        processed_proposal[\"number\"] = {\n",
    "                                          \"value\": proposal[\"proposal_number\"].strip().replace(\"\\t\",\" \"),\n",
    "                                          \"reference_url\": proposal[\"reference_url\"].strip(),\n",
    "                                          \"archive_url\": proposal[\"reference_archive_link\"].strip(),\n",
    "                                          \"retrieved_at\": proposal[\"reference_access_date\"].strip() \n",
    "                                       }\n",
    "    \n",
    "    # Adds the propsal filenumber as \"Aktenzeichen\" property\n",
    "    if 'proposal_filenumber' in proposal.keys():\n",
    "        processed_proposal[\"filenumber\"] = {\n",
    "                                              \"value\": proposal[\"proposal_filenumber\"].strip().replace(\"\\t\",\" \"),\n",
    "                                              \"reference_url\": proposal[\"reference_url\"].strip(),\n",
    "                                              \"archive_url\": proposal[\"reference_archive_link\"].strip(),\n",
    "                                              \"retrieved_at\": proposal[\"reference_access_date\"].strip() \n",
    "                                           }\n",
    "        \n",
    "    # Adds the session.net link as \"Session.net Link\" property\n",
    "    if 'session_net_link' in proposal.keys():\n",
    "        # Gets the reference information for the session.net link\n",
    "        reference_url, archive_url, retrieved_at = get_reference_link_by_proposal_link(proposal[\"session_net_link\"])\n",
    "        processed_proposal[\"session.net_link\"] = {\n",
    "                                                    \"value\": proposal[\"session_net_link\"].strip(),\n",
    "                                                    \"reference_url\": reference_url.strip(),\n",
    "                                                    \"archive_url\": archive_url.strip(),\n",
    "                                                    \"retrieved_at\": retrieved_at.strip() \n",
    "                                                 }\n",
    "    \n",
    "    \n",
    "    # Adds the processed proposal to the list of processed proposals\n",
    "    processed_proposals_for_import.append(processed_proposal)\n",
    "\n",
    "\n",
    "with open('proposals_import.json', 'w', encoding='utf-8') as file_pointer:\n",
    "    json.dump(processed_proposals_for_import, file_pointer, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b503182",
   "metadata": {},
   "source": [
    "## Schritt 5: Erstellung einer JSON-Datei für den Gremiensitzungs-Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e74f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opens the JSON-file with the sessions\n",
    "file_pointer = open(\"sessions_details_list.json\")\n",
    "\n",
    "# Reads the JSON-file with the sessions\n",
    "data = json.load(file_pointer)\n",
    "\n",
    "\n",
    "# Saves the processed sessions for the import\n",
    "processed_sessions_for_import = []\n",
    "\n",
    "\n",
    "# Iterates through the saved sessions\n",
    "for session in data:\n",
    "    processed_session = {}\n",
    "    \n",
    "    # Generates the name of the item\n",
    "    processed_session[\"name_of_item\"] = session[\"session_name\"] + \" (\" + session[\"session_date\"] + \")\"\n",
    "    \n",
    "    # Added the category as \"instenace of\" property\n",
    "    processed_session[\"instance_of\"] = {\n",
    "                                        \"value\": \"Gremiensitzung der Stadt\"\n",
    "                                       }\n",
    "    \n",
    "    # Added the session id as \"\" property\n",
    "    if 'session_id' in session.keys():\n",
    "        processed_session[\"session_number\"] = {\n",
    "                                                \"value\": session[\"session_id\"].strip(),\n",
    "                                                \"reference_url\": session[\"reference\"],\n",
    "                                                \"archive_url\": session[\"reference_archive_link\"],\n",
    "                                                \"retrieved_at\": session[\"reference_access_date\"] \n",
    "                                              }\n",
    "    \n",
    "    \n",
    "    # Added the date of the session as \"Datum\" property\n",
    "    if 'session_date' in session.keys():\n",
    "        processed_session[\"date\"] = {\n",
    "                                          \"value\": session[\"session_date\"].strip(),\n",
    "                                          \"reference_url\": session[\"reference\"],\n",
    "                                          \"archive_url\": session[\"reference_archive_link\"],\n",
    "                                          \"retrieved_at\": session[\"reference_access_date\"] \n",
    "                                         }\n",
    "    \n",
    "    # Added the start time of the session as \"Anfangszeit\" property\n",
    "    if 'session_starttime' in session.keys():\n",
    "        processed_session[\"starttime\"] = {\n",
    "                                          \"value\": session[\"session_starttime\"].strip(),\n",
    "                                          \"reference_url\": session[\"reference\"],\n",
    "                                          \"archive_url\": session[\"reference_archive_link\"],\n",
    "                                          \"retrieved_at\": session[\"reference_access_date\"] \n",
    "                                         }\n",
    "\n",
    "    # Added the end time of the session as \"Endzeit\" property\n",
    "    if 'session_endtime' in session.keys():\n",
    "        processed_session[\"endtime\"] = {\n",
    "                                          \"value\": session[\"session_endtime\"].strip(),\n",
    "                                          \"reference_url\": session[\"reference\"],\n",
    "                                          \"archive_url\": session[\"reference_archive_link\"],\n",
    "                                          \"retrieved_at\": session[\"reference_access_date\"] \n",
    "                                         }\n",
    "        \n",
    "    # Added the session link as \"Session.Net Link\" property\n",
    "    if 'link' in session.keys():\n",
    "        value, reference_url, archive_url, retrieved_at = get_reference_linkinfo_by_session_link(session[\"reference_url\"])\n",
    "        processed_session[\"link\"] = {\n",
    "                                          \"value\": session[\"link\"].strip(),\n",
    "                                          \"reference_url\": reference_url, \n",
    "                                          \"archive_url\": archive_url,\n",
    "                                          \"retrieved_at\": retrieved_at \n",
    "                                         }\n",
    "    \n",
    "    # Added the agenda items as \"Tagesordnungspunkte\" property                                                                                             \n",
    "    if \"agenda\" in session.keys() and len(session[\"agenda\"]) > 0:\n",
    "        processed_session[\"agenda\"] = []\n",
    "        \n",
    "        for item in session[\"agenda\"]:\n",
    "            processed_item = {\n",
    "                                \"public_status\": item[\"public_status\"],\n",
    "                                \"url\": item[\"url_to_proposal\"],\n",
    "                                \"reference_url\": session[\"reference\"],\n",
    "                                \"archive_url\": session[\"reference_archive_link\"],\n",
    "                                \"retrieved_at\": session[\"reference_access_date\"] \n",
    "                             }\n",
    "            \n",
    "            if 'order' in item:\n",
    "                processed_item[\"order\"] = item[\"order\"]\n",
    "            \n",
    "            processed_session[\"agenda\"].append(processed_item)\n",
    "        \n",
    "    \n",
    "    processed_sessions_for_import.append(processed_session)\n",
    "    \n",
    "                                                                                                 \n",
    "with open('session_import.json', 'w', encoding='utf-8') as file_pointer:\n",
    "    json.dump(processed_sessions_for_import, file_pointer, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b691a11",
   "metadata": {},
   "source": [
    "## Schritt 6: Import der Daten in die Wikibase-Datenbank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea7c05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets the login and the password for the wikibase database\n",
    "login, password = get_login_credentials()\n",
    "\n",
    "# Gets the config vailes für the wikibase server\n",
    "mediawiki_api, sparql_endpoint_url, wikibase_url = get_server_address_config()\n",
    "\n",
    "wbi_config['MEDIAWIKI_API_URL'] = mediawiki_api\n",
    "wbi_config['SPARQL_ENDPOINT_URL'] = sparql_endpoint_url\n",
    "wbi_config['WIKIBASE_URL'] = wikibase_url\n",
    "\n",
    "# Creates wikibase integrator instance\n",
    "login_instance = wbi_login.Clientlogin(user=login, password=password)\n",
    "wikiBaseIntegrator = WikibaseIntegrator(login=login_instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c19f5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves the item ids by given item names\n",
    "item_ids = {\n",
    "            \"proposal_category\": \"Q48\",\n",
    "            \"session_category\": \"Q1066\"\n",
    "            }\n",
    "\n",
    "\n",
    "# Saves the property ids by given property names\n",
    "property_ids = {\n",
    "                 \"instance_of\": \"P15\",\n",
    "                 \"propsal_date\": \"P20\",\n",
    "                 \"propsal_type\": \"P24\",\n",
    "                 \"propsal_number\": \"P22\",\n",
    "                 \"filenumber\": \"P21\",\n",
    "                 \"session_net_link\": \"P23\",\n",
    "                 \"propsal_subject\": \"P26\",\n",
    "    \n",
    "                 \"session_number\": \"P28\",\n",
    "                 \"session_date\": \"P27\",\n",
    "                 \"session_starttime\": \"P29\",\n",
    "                 \"session_endtime\": \"P30\",\n",
    "    \n",
    "                 \"agenda_item\": \"P31\",\n",
    "                 \"public_state\": \"P32\",\n",
    "                 \"order\": \"P33\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf8feb2",
   "metadata": {},
   "source": [
    "### Import der Anträge in die Wikibase-Datenbank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc72b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opens the JSON-file with the sessions\n",
    "file_pointer = open(\"proposals_import.json\")\n",
    "\n",
    "# Reads the JSON-file with the sessions\n",
    "data = json.load(file_pointer)\n",
    "\n",
    "\n",
    "# Proposal counter\n",
    "counter = 0\n",
    "\n",
    "# Saves the mapping of proposal link and item id \n",
    "map_between_proposal_and_item_id = {}\n",
    "\n",
    "\n",
    "# Iterates through the proposals\n",
    "for proposal in data:\n",
    "    counter = counter + 1\n",
    "    \n",
    "    # Display the progress message\n",
    "    print('Importing {} of {} ...'.format(str(counter), str(len(data))))\n",
    "    \n",
    "    # Creates a new item for the wikibase database\n",
    "    item = wikiBaseIntegrator.item.new()\n",
    "    \n",
    "    # Saves the properties of the item\n",
    "    properties = []\n",
    "    \n",
    "    print(proposal[\"name_of_item\"])\n",
    "    \n",
    "    # Set a german label of the session or abort the processing\n",
    "    if 'name_of_item' in proposal.keys():\n",
    "        item.labels.set(language='de', value=proposal[\"name_of_item\"][0:250])\n",
    "    else:\n",
    "        print(\"Item can not created because session has no name\")\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    # Add the category \"Gremiensitzung der Stadt Gießen\" as property\n",
    "    instance_of_property = Item(value=item_ids[\"proposal_category\"], prop_nr=property_ids[\"instance_of\"])\n",
    "    properties.append(instance_of_property)\n",
    "    \n",
    "    \n",
    "    for key in proposal.keys():\n",
    "        property_for_proposal = None\n",
    "        \n",
    "        if key == \"date\":\n",
    "            # Build reference from the parlamentsinfo system\n",
    "            references = [get_reference_parlamentsinfosystem(proposal[key][\"reference_url\"], proposal[key][\"archive_url\"], proposal[key][\"retrieved_at\"])]\n",
    "\n",
    "            proposal_date = datetime.fromtimestamp(mktime(time.strptime(proposal[key][\"value\"], \"%d.%m.%Y\")))\n",
    "            proposal_date = proposal_date.strftime(\"+%Y-%m-%dT%H:%M:%SZ\")\n",
    "            property_for_proposal = Time(time=proposal_date, prop_nr=property_ids[\"propsal_date\"], precision=WikibaseDatePrecision.DAY, references=references)\n",
    "        \n",
    "        elif key == \"subject\":\n",
    "            # Build reference from the parlamentsinfo system\n",
    "            references = [get_reference_parlamentsinfosystem(proposal[key][\"reference_url\"], proposal[key][\"archive_url\"], proposal[key][\"retrieved_at\"])]\n",
    "\n",
    "            property_for_proposal = String(value=proposal[key][\"value\"][0:400], prop_nr=property_ids[\"propsal_subject\"], references=references)\n",
    "\n",
    "        elif key == \"type\":\n",
    "            # Build reference from the parlamentsinfo system\n",
    "            references = [get_reference_parlamentsinfosystem(proposal[key][\"reference_url\"], proposal[key][\"archive_url\"], proposal[key][\"retrieved_at\"])]\n",
    "\n",
    "            property_for_proposal = String(value=proposal[key][\"value\"], prop_nr=property_ids[\"propsal_type\"], references=references)\n",
    "        \n",
    "        elif key == \"number\":\n",
    "            # Build reference from the parlamentsinfo system\n",
    "            references = [get_reference_parlamentsinfosystem(proposal[key][\"reference_url\"], proposal[key][\"archive_url\"], proposal[key][\"retrieved_at\"])]\n",
    "\n",
    "            property_for_proposal = String(value=proposal[key][\"value\"], prop_nr=property_ids[\"propsal_number\"], references=references)\n",
    "        \n",
    "        elif key == \"filenumber\":\n",
    "            # Build reference from the parlamentsinfo system\n",
    "            references = [get_reference_parlamentsinfosystem(proposal[key][\"reference_url\"], proposal[key][\"archive_url\"], proposal[key][\"retrieved_at\"])]\n",
    "\n",
    "            property_for_proposal = String(value=proposal[key][\"value\"], prop_nr=property_ids[\"filenumber\"], references=references)\n",
    "        \n",
    "        elif key == \"session.net_link\":\n",
    "            # Build reference from the parlamentsinfo system\n",
    "            references = [get_reference_parlamentsinfosystem(proposal[key][\"reference_url\"], proposal[key][\"archive_url\"], proposal[key][\"retrieved_at\"])]\n",
    "\n",
    "            property_for_proposal = URL(value=proposal[key][\"value\"], prop_nr=property_ids[\"session_net_link\"], references=references)\n",
    "        \n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "     \n",
    "        # Add the property to the property list\n",
    "        properties.append(property_for_proposal)\n",
    "        \n",
    "    \n",
    "    # Add the properties of the property list to the session item\n",
    "    item.claims.add(properties)\n",
    "    \n",
    "    # Writing the item into wikibase database\n",
    "    item.write()\n",
    "    \n",
    "    # Saves the Item ID from the write result\n",
    "    result = item.get_json()\n",
    "    \n",
    "    \n",
    "    url = proposal[\"session.net_link\"][\"value\"]\n",
    "    qid = result[\"id\"]\n",
    "    \n",
    "    if qid not in map_between_proposal_and_item_id:\n",
    "        map_between_proposal_and_item_id[url] = qid\n",
    "        \n",
    "    \n",
    "with open('mapping_proposal_item.json', 'w', encoding='utf-8') as file_pointer:\n",
    "    json.dump(map_between_proposal_and_item_id, file_pointer, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b059aa8",
   "metadata": {},
   "source": [
    "### Import der Gremiensitzungen in die Wikibase-Datenbank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0167e052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opens the JSON-file with the sessions\n",
    "file_pointer = open(\"session_import.json\")\n",
    "\n",
    "# Reads the JSON-file with the sessions\n",
    "data = json.load(file_pointer)\n",
    "\n",
    "\n",
    "# Session counter\n",
    "counter = 0\n",
    "\n",
    "# Gets the mapping of proposal link and item id\n",
    "file_pointer = open(\"mapping_proposal_item.json\")\n",
    "map_between_proposal_and_item_id = json.load(file_pointer)\n",
    "\n",
    "\n",
    "for session in data:\n",
    "    counter = counter + 1\n",
    "    \n",
    "    # Display the progress message\n",
    "    print('Importing {} of {}: {}'.format(str(counter), str(len(data)), session[\"name_of_item\"]))\n",
    "    \n",
    "    # Creates a new item for the wikibase database\n",
    "    item = wikiBaseIntegrator.item.new()\n",
    "    \n",
    "    # Saves the properties of the item\n",
    "    properties = []\n",
    "    \n",
    "    \n",
    "    # Set a german label of the session or abort the processing\n",
    "    if 'name_of_item' in session.keys():\n",
    "        item.labels.set(language='de', value=session[\"name_of_item\"])\n",
    "    else:\n",
    "        print(\"Item can not created because session has no name\")\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    # Add the category \"Gremiensitzung der Stadt Gießen\" as property\n",
    "    instance_of_property = Item(value=item_ids[\"session_category\"], prop_nr=property_ids[\"instance_of\"])\n",
    "    properties.append(instance_of_property)\n",
    "        \n",
    "        \n",
    "    for key in session.keys():\n",
    "        property_for_session = None               \n",
    "        \n",
    "        if key == \"session_number\":\n",
    "            # Build reference from the parlamentsinfo system\n",
    "            references = [get_reference_parlamentsinfosystem(session[key][\"reference_url\"], session[key][\"archive_url\"], session[key][\"retrieved_at\"])]\n",
    "\n",
    "            property_for_session = String(value=session[key][\"value\"], prop_nr=property_ids[\"session_number\"], references=references)\n",
    "\n",
    "        elif key == \"date\":\n",
    "            # Build reference from the parlamentsinfo system\n",
    "            references = [get_reference_parlamentsinfosystem(session[key][\"reference_url\"], session[key][\"archive_url\"], session[key][\"retrieved_at\"])]\n",
    "\n",
    "            session_date = datetime.fromtimestamp(mktime(time.strptime(session[\"date\"][\"value\"], \"%d.%m.%Y\")))\n",
    "            session_date = session_date.strftime(\"+%Y-%m-%dT%H:%M:%SZ\")\n",
    "            property_for_session = Time(time=session_date, prop_nr=property_ids[\"session_date\"], precision=WikibaseDatePrecision.DAY, references=references)\n",
    "        \n",
    "        elif key == \"starttime\":\n",
    "            # Build reference from the parlamentsinfo system\n",
    "            references = [get_reference_parlamentsinfosystem(session[key][\"reference_url\"], session[key][\"archive_url\"], session[key][\"retrieved_at\"])]\n",
    "\n",
    "            property_for_session = String(value=session[key][\"value\"], prop_nr=property_ids[\"session_starttime\"], references=references)\n",
    "            \n",
    "        elif key == \"endtime\":\n",
    "            # Build reference from the parlamentsinfo system\n",
    "            references = [get_reference_parlamentsinfosystem(session[key][\"reference_url\"], session[key][\"archive_url\"], session[key][\"retrieved_at\"])]\n",
    "\n",
    "            property_for_session = String(value=session[key][\"value\"], prop_nr=property_ids[\"session_endtime\"], references=references)\n",
    "        \n",
    "        elif key == \"agenda\":\n",
    "           \n",
    "            for agenda_item in session[key]:\n",
    "                # Build reference from the parlamentsinfo system\n",
    "                references = [get_reference_parlamentsinfosystem(agenda_item[\"reference_url\"], agenda_item[\"archive_url\"], agenda_item[\"retrieved_at\"])]\n",
    "\n",
    "                # Build the qualifiers for the agenda topics\n",
    "                public_state = None\n",
    "                if agenda_item[\"public_status\"] == True:\n",
    "                    public_state = \"öffentlich\"\n",
    "                else:\n",
    "                    public_state = \"nicht öffentlich\"\n",
    "                \n",
    "                qualifiers = [\n",
    "                                String(value=str(agenda_item[\"order\"]), prop_nr=property_ids[\"order\"]),\n",
    "                                String(value=public_state, prop_nr=property_ids[\"public_state\"])\n",
    "                             ]\n",
    "                \n",
    "                url = agenda_item[\"url\"]\n",
    "                \n",
    "                if url not in map_between_proposal_and_item_id:\n",
    "                    print(\"Error: Could not get proposal as item!\")\n",
    "                    continue\n",
    "                \n",
    "                qid = map_between_proposal_and_item_id[url]\n",
    "                property_for_session = Item(value=qid, prop_nr=property_ids[\"agenda_item\"], references=references, qualifiers=qualifiers)\n",
    "                properties.append(property_for_session)\n",
    "                \n",
    "            continue\n",
    "        \n",
    "        elif key == \"session.net_link\":\n",
    "            # Build reference from the parlamentsinfo system\n",
    "            references = [get_reference_parlamentsinfosystem(session[key][\"reference_url\"], session[key][\"archive_url\"], session[key][\"retrieved_at\"])]\n",
    "\n",
    "            property_for_session = URL(value=\"\", prop_nr=property_ids[\"session_net_link\"], references=references)\n",
    "        \n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        # Add the property to the property list\n",
    "        properties.append(property_for_session)\n",
    "    \n",
    "    \n",
    "    # Add the properties of the property list to the session item\n",
    "    item.claims.add(properties)\n",
    "    \n",
    "    # Writing the item into wikibase database\n",
    "    item.write()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
